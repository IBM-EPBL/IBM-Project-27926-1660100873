{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **1. Download the dataset**\n",
    ">\n",
    "> **2. Import requied library**  \n",
    "> import pandas as pd  \n",
    "> import numpy as np  \n",
    "> from sklearn.model_selection import train_test_split  \n",
    "> from sklearn.preprocessing import LabelEncoder  \n",
    "> from keras.models import Model  \n",
    "> from keras.layers import LSTM, Activation, Dense, Dropout,\n",
    "> Input,Embedding  \n",
    "> from keras.optimizers import RMSprop  \n",
    "> from keras.preprocessing.text import Tokenizer  \n",
    "> from keras_preprocessing import sequence  \n",
    "> from keras.utils import to_categorical  \n",
    "> from keras.models import load_model  \n",
    "> import csv  \n",
    "> import tensorflow as tf  \n",
    "> import pandas as pd  \n",
    "> import numpy as np  \n",
    "> import matplotlib.pyplot as plt  \n",
    "> from tensorflow.keras.preprocessing.text import Tokenizer  \n",
    "> from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "> import nltk  \n",
    "> nltk.download('stopwords')  \n",
    "> from nltk.corpus import stopwords  \n",
    "> STOPWORDS = set(stopwords.words('english'))\n",
    ">\n",
    "> \\[nltk_data\\] Downloading package stopwords to /root/nltk_data...\n",
    "> \\[nltk_data\\] Package stopwords is already up-to-date!\n",
    ">\n",
    "> import pandas as pd  \n",
    "> import numpy as np  \n",
    "> import seaborn as sns  \n",
    "> import matplotlib.pyplot as plt  \n",
    "> %matplotlib inline\n",
    ">\n",
    "> **3. Read dataset and do preprocessing**  \n",
    "> df =pd.read_csv('/content/spam.csv',delimiter=',',encoding='latin-1')\n",
    ">\n",
    "> **To see first 5 rows in dataset**  \n",
    "> df.head()\n",
    ">\n",
    "> v1 v2 Unnamed: 2 \\\\\n",
    ">\n",
    "> 0 ham Go until jurong point, crazy.. Available only ... NaN\n",
    ">\n",
    "> 1 ham Ok lar... Joking wif u oni... NaN\n",
    ">\n",
    "> 2 spam Free entry in 2 a wkly comp to win FA Cup fina... NaN\n",
    ">\n",
    "> 3 ham U dun say so early hor... U c already then say... NaN\n",
    ">\n",
    "> 4 ham Nah I don't think he goes to usf, he lives aro... NaN\n",
    ">\n",
    "> Unnamed: 3 Unnamed: 4  \n",
    "> 0 NaN NaN  \n",
    "> 1 NaN NaN  \n",
    "> 2 NaN NaN  \n",
    "> 3 NaN NaN  \n",
    "> 4 NaN NaN\n",
    ">\n",
    "> Dropping unwanted columns\n",
    "\n",
    "df.drop(\\['Unnamed: 2','Unnamed: 3', 'Unnamed: 4'\\],axis=1,inplace=True)\n",
    "\n",
    "> To get summary of the dataset\n",
    ">\n",
    "> df.info()\n",
    ">\n",
    "> \\<class 'pandas.core.frame.DataFrame'\\> RangeIndex: 5572 entries, 0 to\n",
    "> 5571 Data columns (total 2 columns):  \n",
    "> \\# Column Non-Null Count Dtype --- ------ -------------- ----- 0 v1\n",
    "> 5572 non-null object 1 v2 5572 non-null object dtypes: object(2)  \n",
    "> memory usage: 87.2+ KB\n",
    ">\n",
    "> To get Count of Spam and Ham values\n",
    ">\n",
    "> df.groupby(\\['v1'\\]).size()\n",
    ">\n",
    "> v1  \n",
    "> ham 4825  \n",
    "> spam 747  \n",
    "> dtype: int64\n",
    ">\n",
    "> Label Encoding target column\n",
    ">\n",
    "> X = df.v2  \n",
    "> Y = df.v1  \n",
    "> le = LabelEncoder()  \n",
    "> Y = le.fit_transform(Y)  \n",
    "> Y = Y.reshape(-1,1)\n",
    ">\n",
    "> Spliting the data into training set and testing set\n",
    ">\n",
    "> X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)\n",
    ">\n",
    "> Tokenisation function\n",
    ">\n",
    "> max_words =1000  \n",
    "> max_len =150  \n",
    "> tok = Tokenizer(num_words=max_words)  \n",
    "> tok.fit_on_texts(X_train)  \n",
    "> sequences = tok.texts_to_sequences(X_train)  \n",
    "> sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    ">\n",
    "> **4. Creating a model**  \n",
    "> from tensorflow.keras.models import Sequential from\n",
    "> tensorflow.keras.layers import LSTM,Dense\n",
    ">\n",
    "> **5. Adding layers**  \n",
    "> inputs = Input(name='InputLayer',shape=\\[max_len\\])  \n",
    "> layer = Embedding(max_words,50,input_length=max_len)(inputs) layer =\n",
    "> LSTM(64)(layer)  \n",
    "> layer = Dense(256,name='FullyConnectedLayer1')(layer)  \n",
    "> layer = Activation('relu')(layer)  \n",
    "> layer = Dropout(0.5)(layer)  \n",
    "> layer = Dense(1,name='OutputLayer')(layer)  \n",
    "> layer = Activation('sigmoid')(layer)\n",
    ">\n",
    "> model = Model(inputs=inputs,outputs=layer) model.summary()\n",
    ">\n",
    "> Model: \"model_1\"  \n",
    "> \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "> Layer (type) Output Shape Param \\#\n",
    "> =================================================================\n",
    "> InputLayer (InputLayer) \\[(None, 150)\\] 0  \n",
    "> embedding_1 (Embedding) (None, 150, 50) 50000 lstm_1 (LSTM) (None, 64)\n",
    "> 29440 FullyConnectedLayer1 (Dense (None, 256) 16640 )  \n",
    "> activation_2 (Activation) (None, 256) 0  \n",
    "> dropout_1 (Dropout) (None, 256) 0  \n",
    "> OutputLayer (Dense) (None, 1) 257\n",
    ">\n",
    "> activation_3 (Activation) (None, 1) 0  \n",
    "> =================================================================\n",
    "> Total params: 96,337  \n",
    "> Trainable params: 96,337  \n",
    "> Non-trainable params: 0  \n",
    "> \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    ">\n",
    "> **6. Compile the model**  \n",
    "> model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=\\[\n",
    "> 'accuracy'\\])\n",
    ">\n",
    "> **7. Fit the model**  \n",
    "> model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n",
    "> validation_split=0.2)\n",
    ">\n",
    "> Epoch 1/10  \n",
    "> 30/30 \\[==============================\\] - 13s 329ms/step - loss:\n",
    "> 0.3208- accuracy: 0.8809 - val_loss: 0.1565 - val_accuracy: 0.9768  \n",
    "> Epoch 2/10  \n",
    "> 30/30 \\[==============================\\] - 10s 324ms/step - loss:\n",
    "> 0.0800- accuracy: 0.9805 - val_loss: 0.0696 - val_accuracy: 0.9810  \n",
    "> Epoch 3/10  \n",
    "> 30/30 \\[==============================\\] - 9s 296ms/step - loss:\n",
    "> 0.0398 - accuracy: 0.9892 - val_loss: 0.0829 - val_accuracy: 0.9768  \n",
    "> Epoch 4/10  \n",
    "> 30/30 \\[==============================\\] - 9s 303ms/step - loss:\n",
    "> 0.0277 - accuracy: 0.9913 - val_loss: 0.0848 - val_accuracy: 0.9747  \n",
    "> Epoch 5/10  \n",
    "> 30/30 \\[==============================\\] - 9s 305ms/step - loss:\n",
    "> 0.0199 - accuracy: 0.9939 - val_loss: 0.0893 - val_accuracy: 0.9768  \n",
    "> Epoch 6/10  \n",
    "> 30/30 \\[==============================\\] - 10s 344ms/step - loss:\n",
    "> 0.0122- accuracy: 0.9966 - val_loss: 0.0969 - val_accuracy: 0.9810  \n",
    "> Epoch 7/10  \n",
    "> 30/30 \\[==============================\\] - 9s 302ms/step - loss:\n",
    "> 0.0111 - accuracy: 0.9966 - val_loss: 0.1346 - val_accuracy: 0.9736  \n",
    "> Epoch 8/10  \n",
    "> 30/30 \\[==============================\\] - 9s 299ms/step - loss:\n",
    "> 0.0075 - accuracy: 0.9979 - val_loss: 0.1451 - val_accuracy: 0.9715  \n",
    "> Epoch 9/10  \n",
    "> 30/30 \\[==============================\\] - 9s 305ms/step - loss:\n",
    "> 0.0070 - accuracy: 0.9979 - val_loss: 0.1349 - val_accuracy: 0.9810  \n",
    "> Epoch 10/10  \n",
    "> 30/30 \\[==============================\\] - 9s 304ms/step - loss:\n",
    "> 0.0057 - accuracy: 0.9989 - val_loss: 0.1549 - val_accuracy: 0.9821\n",
    ">\n",
    "> \\<keras.callbacks.History at 0x7f40ee615c50\\>\n",
    ">\n",
    "> **8. Save the model**  \n",
    "> model.save(\"model_1\")\n",
    ">\n",
    "> WARNING:absl:Function \\`\\_wrapped_model\\` contains input name(s)\n",
    "> InputLayer with unsupported characters which will be renamed to\n",
    "> inputlayer in the SavedModel.\n",
    ">\n",
    "> WARNING:absl:Found untraced functions such as  \n",
    "> lstm_cell_1\\_layer_call_fn,  \n",
    "> lstm_cell_1\\_layer_call_and_return_conditional_losses while saving\n",
    "> (showing 2 of 2). These functions will not be directly callable after\n",
    "> loading.\n",
    ">\n",
    "> test_sequences = tok.texts_to_sequences(X_test)  \n",
    "> test_sequences_matrix  \n",
    "> =sequence.pad_sequences(test_sequences,maxlen=max_len) accuracy =\n",
    "> model.evaluate(test_sequences_matrix,Y_test) print('Accuracy:\n",
    "> {:0.3f}'.format(accuracy\\[1\\]))\n",
    ">\n",
    "> 27/27 \\[==============================\\] - 1s 26ms/step - loss: 0.1476\n",
    "> accuracy: 0.9809  \n",
    "> Accuracy: 0.981\n",
    ">\n",
    "> y_pred = model.predict(test_sequences_matrix)\n",
    "> print(y_pred\\[25:40\\].round(3))\n",
    ">\n",
    "> 27/27 \\[==============================\\] - 1s 26ms/step \\[\\[0. \\]  \n",
    "> \\[0. \\]  \n",
    "> \\[0.998\\]  \n",
    "> \\[0. \\]  \n",
    "> \\[0.001\\]  \n",
    "> \\[0. \\]  \n",
    "> \\[0. \\]  \n",
    "> \\[0. \\]  \n",
    "> \\[0. \\]  \n",
    "> \\[0. \\]  \n",
    "> \\[0.001\\]  \n",
    "> \\[1. \\]  \n",
    "> \\[0. \\]  \n",
    "> \\[1. \\]  \n",
    "> \\[0. \\]\\]\n",
    ">\n",
    "> **9. Test the model**  \n",
    "> print(Y_test\\[25:40\\])\n",
    ">\n",
    "> \\[\\[0\\]  \n",
    "> \\[0\\]\n",
    ">\n",
    "> \\[1\\]  \n",
    "> \\[0\\]  \n",
    "> \\[0\\]  \n",
    "> \\[0\\]  \n",
    "> \\[0\\]  \n",
    "> \\[0\\]  \n",
    "> \\[0\\]  \n",
    "> \\[0\\]  \n",
    "> \\[0\\]  \n",
    "> \\[1\\]  \n",
    "> \\[0\\]  \n",
    "> \\[1\\]  \n",
    "> \\[0\\]\\]"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
